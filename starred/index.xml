<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Starreds on Qiu&#39;s Quibble</title>
    <link>http://blog.idempotent.ca/starred/</link>
    <description>Recent content in Starreds on Qiu&#39;s Quibble</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Sep 2015 22:16:36 -0400</lastBuildDate>
    <atom:link href="http://blog.idempotent.ca/starred/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Use Python bytecode to solve puzzler</title>
      <link>http://blog.idempotent.ca/starred/2015-09-03-use-python-bytecode-to-solve-puzzler/</link>
      <pubDate>Thu, 03 Sep 2015 22:16:36 -0400</pubDate>
      
      <guid>http://blog.idempotent.ca/starred/2015-09-03-use-python-bytecode-to-solve-puzzler/</guid>
      <description>

&lt;h2 id=&#34;learning-python-internals&#34;&gt;Learning Python Internals&lt;/h2&gt;

&lt;p&gt;Recently I stumbled upon &lt;a href=&#34;https://www.youtube.com/playlist?list=PLwyG5wA5gIzgTFj5KgJJ15lxq5Cv6lo_0&#34;&gt;this wonderful set of videos on Python interpreter internals&lt;/a&gt;. (Thanks to &lt;a href=&#34;http://pgbovine.net/&#34;&gt;Philip Guo&lt;/a&gt; for creating them and thanks to Michael Kennedy (@mkennedy) and his &lt;a href=&#34;http://talkpython.fm/&#34;&gt;Talk Python to me&lt;/a&gt; show that brought this on my radar)&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been using Python for about ten years but I&amp;rsquo;ve never really truly been able to understand how the interpreter works, nor was I familiar with the Python virtual machine or the bytecode. These videos may just be the extra help I needed to get me started at the internals of Python.&lt;/p&gt;

&lt;p&gt;So far, I&amp;rsquo;ve only watched 2 lectures and I&amp;rsquo;m already learning a lot. I learned where to find a list of opcodes in the source code, where the main eval loop is, and what internal states the Python virtual machine keeps.&lt;/p&gt;

&lt;p&gt;Then I thought to myself, why not use this new found power to solve some Python mysterious that have been puzzling me?&lt;/p&gt;

&lt;h2 id=&#34;the-puzzler&#34;&gt;The puzzler&lt;/h2&gt;

&lt;p&gt;A few days ago, one of my former co-workers posted this puzzler:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(a, b) = a[b] = {}, 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What are the values of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; after the assignment? Well, it&amp;rsquo;s not obvious what the order of assignment it is going to be. Putting it in the REPL gives us this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; (a, b) = a[b] = {}, 5
&amp;gt;&amp;gt;&amp;gt; a
{5: ({...}, 5)}
&amp;gt;&amp;gt;&amp;gt; a[5]
({5: ({...}, 5)}, 5)
&amp;gt;&amp;gt;&amp;gt; a[5][0]
{5: ({...}, 5)}
&amp;gt;&amp;gt;&amp;gt; a[5][0][5]
({5: ({...}, 5)}, 5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK, so there appears to be a circular reference going on here. The object that &lt;code&gt;a&lt;/code&gt; refers to has an element that refers to the object that &lt;code&gt;a&lt;/code&gt; refers to and so on and so forth. Now, the question is, how did the circular reference get there?&lt;/p&gt;

&lt;p&gt;Well, all Python source code eventually get compiled down to bytecode and executed on the virtual machine. In order to understand what that line actually does, we need to look at the byte code.&lt;/p&gt;

&lt;p&gt;It turns out that Python comes with a module to disassemble source code into byte codes (assembly for the virtual machine):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python -m dis
a, b = a[5] = {}, 5
^D
  1           0 BUILD_MAP                0
              3 LOAD_CONST               0 (5)
              6 BUILD_TUPLE              2
              9 DUP_TOP
             10 UNPACK_SEQUENCE          2
             13 STORE_NAME               0 (a)
             16 STORE_NAME               1 (b)
             19 LOAD_NAME                0 (a)
             22 LOAD_CONST               0 (5)
             25 STORE_SUBSCR
             26 LOAD_CONST               1 (None)
             29 RETURN_VALUE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alright, so that humble little line of code is actually 12 instructions for the Python virtual machine. Each instruction manipulates the virtual machine&amp;rsquo;s internal state in some way. CPython is a stack-based interpreter, which means certain instructions puts values on the stack and other instructions consume them from the stack.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go through the instructions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0 BUILD_MAP                0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First off, it tells the interpreter to make a map object and put it on the value stack. After this instruction, our value stack looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+----+
| {} |
+----+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next up:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3 LOAD_CONST               0 (5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This loads a constant (&lt;code&gt;5&lt;/code&gt;) on the stack.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+----+
| {} |
+----+
| 5  |
+----+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;6 BUILD_TUPLE              2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This instruction builds a &lt;code&gt;PyTuple&lt;/code&gt; object of size &lt;code&gt;2&lt;/code&gt;, which is in the argument of the opcode. It consumes the top &lt;code&gt;2&lt;/code&gt; things on the stack and make a 2-tuple using these values and put the result tuple on the value stack:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+---------+
| ({}, 5) |
+---------+
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;9 DUP_TOP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we have the &lt;code&gt;DUP_TOP&lt;/code&gt; instruction. It probably stands for &amp;ldquo;duplicate the top of the stack&amp;rdquo;, and reading the corresponding code in the eval loop, this seems to be what it&amp;rsquo;s doing: it gets the object from the top of the stack without popping it off and push the value on the stack, while incrementing the refcount of the object.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s worth noting that this only duplicates the tuple object. The elements inside the tuple are of type &lt;code&gt;*PyObject&lt;/code&gt;, which are pointers to the corresponding values (the dict and the integer), and are not duplicated by this instruction. Here&amp;rsquo;s the value stack after this instruction:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+---------+
| ({}, 5) |
+---------+
| ({}, 5) |
+---------+
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;10 UNPACK_SEQUENCE          2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next instruction is &lt;code&gt;UNPACK_SEQUENCE&lt;/code&gt; with argument &lt;code&gt;2&lt;/code&gt;. This will first pop the stack, so &lt;code&gt;({}, 5)&lt;/code&gt; is off the stack, and then push each element from the tuple on the stack in reverse order. After this instruction, the stack will be:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+---------+
| ({}, 5) |
+---------+
|   5     |
+---------+
|   {}    |
+---------+
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;13 STORE_NAME               0 (a)
16 STORE_NAME               1 (b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next two instructions deal with &amp;ldquo;names&amp;rdquo;, which are variables for the scope of the frame. &lt;code&gt;STORE_NAME a&lt;/code&gt; will pop the stack, and point &lt;code&gt;a&lt;/code&gt; to the value, and similarily for &lt;code&gt;STORE_NAME b&lt;/code&gt;. After this instruction, there will be two bindings in the frame: &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; and the stack will be back to having only one element, the tuple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stack:
+---------+
| ({}, 5) |
+---------+

bindings:
a &amp;lt;- {}
b &amp;lt;- 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next two instructions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;19 LOAD_NAME                0 (a)
22 LOAD_CONST               0 (5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;LOAD_NAME a&lt;/code&gt; will push the value that the variable is bound to on the stack, so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stack:
+---------+
| ({}, 5) |
+---------+
|    {}   |
+---------+

bindings:
a &amp;lt;- {}
b &amp;lt;- 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and &lt;code&gt;LOAD_CONST 5&lt;/code&gt;, as we&amp;rsquo;ve seen before, simply pushes the constant &lt;code&gt;5&lt;/code&gt; on the stack:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stack:
+---------+
| ({}, 5) |
+---------+
|    {}   |
+---------+
|     5   |
+---------+

bindings:
a &amp;lt;- {}
b &amp;lt;- 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;25 STORE_SUBSCR
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is where the magic happens. &lt;code&gt;STORE_SUBSCR&lt;/code&gt; is an instruction to set element on the dictionary given the index. Here&amp;rsquo;s the code that handles this opcode in the eval loop:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TARGET_NOARG(STORE_SUBSCR)
{
    w = TOP();
    v = SECOND();
    u = THIRD();
    STACKADJ(-3);
    /* v[w] = u */
    err = PyObject_SetItem(v, w, u);
    Py_DECREF(u);
    Py_DECREF(v);
    Py_DECREF(w);
    if (err == 0) DISPATCH();
    break;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, &lt;code&gt;TOP&lt;/code&gt;, &lt;code&gt;SECOND&lt;/code&gt;, &lt;code&gt;THIRD&lt;/code&gt; are macros that take values off of the value stack. Given our state of the virtual machine:
* &lt;code&gt;w = TOP()&lt;/code&gt; =&amp;gt; &lt;code&gt;w = 5&lt;/code&gt;
* &lt;code&gt;v = SECOND()&lt;/code&gt; =&amp;gt; &lt;code&gt;v = {}&lt;/code&gt;
* &lt;code&gt;w = THIRD()&lt;/code&gt; =&amp;gt; &lt;code&gt;w = ({}, 5)&lt;/code&gt;, but keep in mind, the first element in &lt;code&gt;w&lt;/code&gt; (the tuple) is actually the same object &lt;code&gt;v&lt;/code&gt; is pointing to.&lt;/p&gt;

&lt;p&gt;Thus, calling &lt;code&gt;PyObject_SetItem(v, w, u)&lt;/code&gt; sets &lt;code&gt;v[w] = u&lt;/code&gt; =&amp;gt; &lt;code&gt;v[5] = (v, 5)&lt;/code&gt;, and there a circular reference is born!&lt;/p&gt;

&lt;p&gt;From the sequence of operation, we can tell the order by which the assignments were executed:
1. &lt;code&gt;a, b = {}, 5&lt;/code&gt;
2. &lt;code&gt;a[5] = ({}, 5)&lt;/code&gt;, with &lt;code&gt;a&lt;/code&gt; refering to the dictionary&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Diving into the Python implementation is the next level ninjary that may come in handy in some cases. Granted, no one is going to write production code like the one in the puzzler, but stepping through and visualizing the virtual machine is a pretty useful and fun experience that makes me appreciate more the language I use everyday.&lt;/p&gt;

&lt;p&gt;Again, thanks to Philip Guo for the videos and Michael Kennedy for the podcast. Also, checkout Professor Guo&amp;rsquo;s &lt;a href=&#34;http://www.pythontutor.com/&#34;&gt;python tutor&lt;/a&gt; for visualizing how code is run.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use rabbitmq DLX to implement delayed retry</title>
      <link>http://blog.idempotent.ca/starred/2015-04-30-use-rabbitmq-dlx-to-implement-delayed-retry/</link>
      <pubDate>Thu, 30 Apr 2015 00:37:42 -0400</pubDate>
      
      <guid>http://blog.idempotent.ca/starred/2015-04-30-use-rabbitmq-dlx-to-implement-delayed-retry/</guid>
      <description>

&lt;p&gt;In this post, I&amp;rsquo;m going to describe the experience at &lt;code&gt;$DAYJOB&lt;/code&gt; regarding implementing delayed retry using &lt;a href=&#34;https://www.rabbitmq.com/&#34;&gt;rabbitmq&lt;/a&gt;&amp;rsquo;s &lt;a href=&#34;https://www.rabbitmq.com/dlx.html&#34;&gt;DLX&lt;/a&gt; combined with a TTL. The technique has been described at a few &lt;a href=&#34;http://yuserinterface.com/dev/2013/01/08/how-to-schedule-delay-messages-with-rabbitmq-using-a-dead-letter-exchange/&#34;&gt;places&lt;/a&gt; but it is new to me personally and our company. I&amp;rsquo;d like to capture the experience we had both in implementing and in deploying to production.&lt;/p&gt;

&lt;h1 id=&#34;the-problem&#34;&gt;The problem&lt;/h1&gt;

&lt;p&gt;At &lt;code&gt;$DAYJOB&lt;/code&gt; we have a service that integrates with a 3rd-party API that processes credit card payments and when successful, records a payment object on our customer&amp;rsquo;s invoices, and change the invoice status. Pretty straight-forward stuff. However, lately we&amp;rsquo;ve been experiencing an elevated amount of random failures from our service provider.&lt;/p&gt;

&lt;p&gt;Calls to our provider to create a checkout using the client&amp;rsquo;s credit card information would time out randomly, or return an &amp;ldquo;unknown error&amp;rdquo;. When it happens, we don&amp;rsquo;t record a payment object on the invoice since we don&amp;rsquo;t know the actual status of the checkout, nor do we have the &lt;code&gt;reference_id&lt;/code&gt; for the checkout. However, as we discovered, some of these timed-out calls did go through and the clients&amp;rsquo; credit cards charged.&lt;/p&gt;

&lt;p&gt;We checked with our service provider and were told that they have been experiencing increased volumes and their infrastructure currently can&amp;rsquo;t keep up. However, they suggest that we use an undocumented feature which allows a &lt;code&gt;unique_id&lt;/code&gt; to be passed in along with the checkout call. The &lt;code&gt;unique_id&lt;/code&gt; serves as an idempotent key (similar to &lt;a href=&#34;https://stripe.com/docs/api?lang=curl#idempotent_requests&#34;&gt;Stripe&amp;rsquo;s&lt;/a&gt;). Multiple calls with the same &lt;code&gt;unique_id&lt;/code&gt; won&amp;rsquo;t create multiple checkout objects on their end and thus ensuring the checkout is made but won&amp;rsquo;t double/triple charge the customer&amp;rsquo;s car.&lt;/p&gt;

&lt;h1 id=&#34;architecting-the-solution&#34;&gt;Architecting the solution&lt;/h1&gt;

&lt;p&gt;Armed with this new secret API feature, our team goes back to the drawing board. At work, we use &lt;a href=&#34;https://www.rabbitmq.com/&#34;&gt;rabbitmq&lt;/a&gt; extensively for asynchronous processing. If some operation doesn&amp;rsquo;t have to be carried out synchronously with a web request, we throw a message on the queue and have a queue consumer process that message and update states. We use a library called &lt;a href=&#34;https://github.com/ojacobson/sparkplug&#34;&gt;sparkplug&lt;/a&gt; that makes writing queue consumer super-easy. So, everything seems to fall in friendly terrotiries: we make a checkout call with a random id and when we encounter timeout or unknown error, instead of returning an error response to the user, we return &lt;code&gt;202 Accepted&lt;/code&gt; to our user and throw a message on the queue, so a consumer can grab it and retry the checkout with the same original &lt;code&gt;unique_id&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&#34;the-missing-piece&#34;&gt;The missing piece&lt;/h1&gt;

&lt;p&gt;However, we quickly realized it&amp;rsquo;s not that simple. What if the retry encountered the same error? We can put it back on the queue, but when does it get processed by the consumer again? We want to add a time delay to the subsequent retries, and the orginal retry as well.&lt;/p&gt;

&lt;h1 id=&#34;dead-letter-exchange-https-www-rabbitmq-com-dlx-html-and-ttl-https-www-rabbitmq-com-ttl-html&#34;&gt;&lt;a href=&#34;https://www.rabbitmq.com/dlx.html&#34;&gt;Dead-Letter-Exchange&lt;/a&gt; and &lt;a href=&#34;https://www.rabbitmq.com/ttl.html&#34;&gt;TTL&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;After some research on the internet, seems like this problem has been &lt;a href=&#34;https://www.cloudamqp.com/docs/delayed-messages.html&#34;&gt;solved&lt;/a&gt; &lt;a href=&#34;http://yuserinterface.com/dev/2013/01/08/how-to-schedule-delay-messages-with-rabbitmq-using-a-dead-letter-exchange/&#34;&gt;before&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The idea here is that you have two queues: &lt;code&gt;Qa&lt;/code&gt; and &lt;code&gt;Qb&lt;/code&gt;.  When a checkout request times out, we put a message on a &lt;code&gt;Qa&lt;/code&gt;.  &lt;code&gt;Qa&lt;/code&gt; is declared with &lt;code&gt;x-dead-letter-exchange&lt;/code&gt;, &lt;code&gt;x-dead-letter-routing-key&lt;/code&gt; and &lt;code&gt;x-message-ttl&lt;/code&gt; (in milliseconds).  When the message is in &lt;code&gt;Qa&lt;/code&gt; for &lt;code&gt;ttl&lt;/code&gt; milliseconds, the message will be re-routed to the specified dead-letter-exchange with the routing key.  We can bind &lt;code&gt;Qb&lt;/code&gt; to the exchange with the routing key, and attach a consumer to only &lt;code&gt;Qb&lt;/code&gt; and retry the checkout call.&lt;/p&gt;

&lt;p&gt;If the retry call fails for the same reason (timeout or unknown error), we re-publish the message to &lt;code&gt;Qa&lt;/code&gt; again and acknowledges the message so it&amp;rsquo;s no longer in &lt;code&gt;Qb&lt;/code&gt;.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://blog.idempotent.ca/images/dlx_1.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Flow diagram&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The whole flow looks like this:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://blog.idempotent.ca/images/dlx_2.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Flow diagram&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;h1 id=&#34;implementation-testing-strategy-and-deployment-saga&#34;&gt;Implementation, Testing Strategy and Deployment saga&lt;/h1&gt;

&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;Implementation is probably the most straight-forward phase of the project once we have the design on paper.  The only obstacle is that the library we use for writing rabbitmq consumers (sparkplug) does not support declaring queues with extra parameters, and the DLX related parameters: &lt;code&gt;x-dead-letter-exchange&lt;/code&gt; &lt;code&gt;x-dead-letter-routing-key&lt;/code&gt;, and &lt;code&gt;x-message-ttl&lt;/code&gt; are all &amp;ldquo;extra parameters&amp;rdquo; according to &lt;code&gt;amqplib&lt;/code&gt;, which is used by sparkplug. To solve this, I sent this &lt;a href=&#34;https://github.com/ojacobson/sparkplug/pull/10/files&#34;&gt;PR&lt;/a&gt; to sparkplug, so it recognizes extra parameters and pass them down to amqp library.&lt;/p&gt;

&lt;p&gt;Another road block appeared when we ran the system on our dev images for the first time. The underlying amqplib would error out on startup. Upon closer investigation, it appeared the error happened while talking to rabbitmq and the amqplib can&amp;rsquo;t handle certain rabbitmq frames. So I went searching for the amqp project, only to find out that it was deprecated &lt;a href=&#34;https://pypi.python.org/pypi/amqplib&#34;&gt;long ago&lt;/a&gt;. Fortunately, there&amp;rsquo;s a fork of the library &lt;a href=&#34;https://pypi.python.org/pypi/amqp&#34;&gt;amqp&lt;/a&gt; that&amp;rsquo;s maintained by the reputable &lt;a href=&#34;http://www.celeryproject.org/&#34;&gt;Celery project&lt;/a&gt;. It&amp;rsquo;s has API compatibility with amqplib and appeared to be a drop-in replacement. We dropped it in and everything seems to work. Reading the online literature, it seems to be the case that the old library does not handle the &lt;code&gt;TTL&lt;/code&gt; amqp extension.&lt;/p&gt;

&lt;h2 id=&#34;testing-strategy&#34;&gt;Testing Strategy&lt;/h2&gt;

&lt;p&gt;So, since the 3rd party API timeout is an edge case, they did not provide a way trigger this behaviour the same way we can trigger, say, a declined transaction. We could fake the URL for the 3rd party service in DNS or &lt;code&gt;/etc/hosts&lt;/code&gt; or we can change the SDK to change the base url for their API to somewhere else and cause a timeout that way, but neither is ideal. The biggest disadvantage is that we have no way of getting a request out of the retry state.&lt;/p&gt;

&lt;p&gt;Eventually, we decided to &lt;a href=&#34;http://en.wikipedia.org/wiki/Man-in-the-middle_attack&#34;&gt;MITM&lt;/a&gt; ourselves :) We can write a simple proxy server, and for the most part, it&amp;rsquo;s going to be a pass-through, but on certain requests, we intercept it and return an unknown error (500 series with specific response body).  To trigger it, we set the checkout amount to &lt;code&gt;$666&lt;/code&gt;, and in the proxy, we keep an internal counter based on the checkout&amp;rsquo;s unique id, and increment the counter every time it&amp;rsquo;s retried, and then we can set a max retry threshold in the proxy so the proxy becomes a pass through again if the max retry threshold is reached.&lt;/p&gt;

&lt;p&gt;We used this small nifty library &lt;a href=&#34;https://github.com/allfro/pymiproxy&#34;&gt;pymiproxy&lt;/a&gt; as a base for our proxy server. It turns out the proxy is pretty straight-forward as well, and a big shout-out to the author of pymiproxy.&lt;/p&gt;

&lt;h2 id=&#34;deployment&#34;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;Everything until now is like a cake walk. Sure, there are some problems with the underlying libraries but that requires patching but they were quite easy to identify and fix. Deployment, on the other hand, has been like riding on the &lt;a href=&#34;https://www.youtube.com/watch?v=Mgsbau5qkTE&#34;&gt;Behemoth in Canada&amp;rsquo;s Wonderland&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First of all, while getting the code onto the testing environment, we encountered the first gremlin. The staging is running on the exact same version of rabbitmq and the exact same configuration. However, on staging, when a message is published on the DLQ (&lt;code&gt;Qa&lt;/code&gt;) in our example, after &lt;code&gt;TTL&lt;/code&gt;, the message would simply disappear and did not get routed to &lt;code&gt;Qb&lt;/code&gt;. What&amp;rsquo;s worse, sometimes even &lt;code&gt;Qa&lt;/code&gt; is completely gone after the message is dropped on the floor! This is terribly frustrating. The queue is declared as durable, and so is the exchange. I even did a side-by-side comparison of the sparkplug log output to see if anything is different. Well, there was! The declaration sequence is different between staging and dev. On dev, the dead-letter exchange is declared before &lt;code&gt;Qa&lt;/code&gt; which specifies &lt;code&gt;x-dead-letter-exchange&lt;/code&gt;. That makes sense! Reading the &lt;a href=&#34;https://github.com/ojacobson/sparkplug/blob/master/sparkplug/config/__init__.py#L57-L77&#34;&gt;sparkplug code&lt;/a&gt;, it calculates the dependencies between queues, exchanges, bindings and consumers to determine the order of which they should be declared. However, our modification that enabled sparkplug to pass down DLX, but sparkplug has no idea that the queue depends on the DLX! Based on this observation, I cooked up another &lt;a href=&#34;https://github.com/freshbooks/sparkplug/pull/2/files&#34;&gt;PR&lt;/a&gt; such that if DLX is specified, make sure we make the DLX a dependency of the queue so the exchange gets declared before it. Did a few tests locally, and hey, it appears to be working!&lt;/p&gt;

&lt;p&gt;Just as I thought my shrewed observation has solved this major mystery, the second day, people reported that the queue started go AWOL again! Grumbled, I sat down and read carefully the documentation on &lt;a href=&#34;https://www.rabbitmq.com/dlx.html&#34;&gt;dead-letter exchange&lt;/a&gt; and discovered this:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that the exchange does not have to be declared when the queue is declared, but it should exist by the time messages need to be dead-lettered; if it is missing then, the messages will be silently dropped.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This invalidates my previous hypothesis that the out-of order declaration was the root cause of the problem. There we go, I was back to square one.&lt;/p&gt;

&lt;p&gt;At this time, I wanted to try a different approach. Instead of forming hypothesis from observation, I searched for evidence. I went on the server, and start to look at the logs to search for any traces that can be salvaged. The rabbitmq log is very noisy with all the connection messages. Once in a while you get something remotely interesting, but they were not relevant. Then I manually published a message on the queue, and waited for the message and queue to disappear. Lo and behold, there&amp;rsquo;s something in the logs!&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/kevinjqiu/e626bcc40eb803214968.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;There&amp;rsquo;s our smoking gun! Further gooling revealed &lt;a href=&#34;http://lists.rabbitmq.com/pipermail/rabbitmq-discuss/2012-April/019368.html&#34;&gt;this&lt;/a&gt;. That&amp;rsquo;s EXACTLY our issue! And the version of rabbitmq we&amp;rsquo;re using is EXACTLY 2.8.1! What a relief! We just need to upgrade to 2.8.2 and everything would be fine.&lt;/p&gt;

&lt;p&gt;So there I was, preparing an internal repository to host the rpm (since we&amp;rsquo;re on a hopelessly old version of CentOS), and prepared puppet changes for the new version. Deployed on all the environments and sent it off to QA. QA ok&amp;rsquo;ed it just before the weekend and life is good again.&lt;/p&gt;

&lt;p&gt;Except, not at all! There are a few more surprises waiting for us before the end of tunnel. First of all, our partner whose payment API we&amp;rsquo;re integrating has received an imminent DDOS threat, and fearing not having a retry mechanism would caused a huge burden for us and our support crew, we need to get this out to production ASAP. After pulling some levers and convincing our ops team that this is a relatively low risk point release upgrade (from rabbitmq 2.8.1 to 2.8.2), we got the green light and ops are on their way upgrading rabbitmq. Everything seemed to be going alone well, until, when we switched all components to point to the hosts that&amp;rsquo;s on the new rabbitmq, our app stopped working! Phone calls flooded in, alerts set off everywhere and on top of that, even the streets in front of our building had a couple of emergency vehicles passing by! Goodness, what have we done! Ops quickly rolled it back, and we were left dumbfounded by this yet another surprise.&lt;/p&gt;

&lt;p&gt;Analyzing the logs from various components during the downtime, it appeared the components talking to rabbitmq have timed out trying to publish messages. We checked that the hosts can indeed reach each other, all the names can be resolved and firewall rules are not in effect. So, we hit a wall again.&lt;/p&gt;

&lt;p&gt;On the second day, we regrouped, and experimented on the backup data centre. We upgraded, and tried to put a message on the queue, and guess what, it blocked! It&amp;rsquo;s great that we reproduced the issue. Since the staging environment worked just fine, I captured &lt;code&gt;strace&lt;/code&gt; on the staging environment, and ops did the same on prod, and compared the output. It&amp;rsquo;s pretty clear that the process was waiting on reading socket (syscall was &lt;code&gt;recvfrom(...)&lt;/code&gt;) and it blocked. Then I did &lt;code&gt;tcpdump&lt;/code&gt; and compared that with the output on prod, and also proven to be futile.&lt;/p&gt;

&lt;p&gt;In that afternoon, our fortune suddenly took a positive turn, when one of the ops discovered this in the logs while starting the new rabbitmq:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;=INFO REPORT==== 29-Apr-2015::14:51:09 ===
Disk free space limit now exceeded. Free bytes:19033128960 Limit:50634379264
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, this version of rabbitmq started to check free disk space, and &lt;strong&gt;blocks&lt;/strong&gt; incoming message if the disk space is deemed inadequate! Wow, this is so unexpected that we all laughed when we discovered this to be the root cause. However, for me, I need to be convinced that why it wasn&amp;rsquo;t an issue for staging environment.&lt;/p&gt;

&lt;p&gt;So I cloned rabbitmq git repository, and looked for anything that&amp;rsquo;s related to &lt;code&gt;disk_free_limit&lt;/code&gt;. Finally, I found this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-erlang&#34;&gt;{disk_free_limit, {mem_relative, 1.0}},
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;from &lt;a href=&#34;https://github.com/rabbitmq/rabbitmq-server/blob/rabbitmq_v2_8_2/ebin/rabbit_app.in#L22&#34;&gt;here&lt;/a&gt;. Since we&amp;rsquo;re using the default config, this is in effect, and it essentially says &amp;ldquo;stop accepting message if the disk space is not at least as big as the RAM&amp;rdquo;, and it just so happens on prod, we have 50G of RAM and therefore, require at least 50G of free space for rabbitmq to start accepting messages!&lt;/p&gt;

&lt;p&gt;Reading the rabbitmq 2.8.2 release notes, and they &lt;strong&gt;did&lt;/strong&gt; &lt;a href=&#34;https://www.rabbitmq.com/release-notes/README-2.8.2.txt&#34;&gt;mention&lt;/a&gt; this &amp;ldquo;feature&amp;rdquo;, but failed to mention that it could block your connection &lt;strong&gt;forever&lt;/strong&gt; and bring your site down&amp;hellip;&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;There you go.  That&amp;rsquo;s our adventure implementing and deploying delayed retry using rabbitmq&amp;rsquo;s DLX and TTL. It&amp;rsquo;s frustrating and rewarding at the same time, and there&amp;rsquo;s definitely something we can all take home with:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Software is hard, even for experienced developers and ops&lt;/li&gt;
&lt;li&gt;Gather all the evidences before forming hypothesis on the root cause&lt;/li&gt;
&lt;li&gt;Certainly, read the docs thoroughly before hypothesizing&lt;/li&gt;
&lt;li&gt;Expect problems when switching environments&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I haven&amp;rsquo;t been blogging for a while, partly because life catches up, and partly because I&amp;rsquo;ve been less than disciplined but I spent some time writing down this experience worth remembering :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fast and elegant way to sum primes in a gigantic range</title>
      <link>http://blog.idempotent.ca/starred/2009-06-01-fast-and-elegant-way-to-sum-primes-in-a-gigantic-range/</link>
      <pubDate>Mon, 01 Jun 2009 09:37:00 +0000</pubDate>
      
      <guid>http://blog.idempotent.ca/starred/2009-06-01-fast-and-elegant-way-to-sum-primes-in-a-gigantic-range/</guid>
      <description>

&lt;p&gt;The problem is taken from &lt;a href=&#34;http://projecteuler.net/&#34;&gt;Project-Euler&lt;/a&gt;, which asks what is the sum of all prime numbers under 2 million.&lt;/p&gt;

&lt;h2 id=&#34;traditional-approach&#34;&gt;Traditional Approach&lt;/h2&gt;

&lt;p&gt;Project-Euler has many problems like this which looks ridiculously easy in theory, but practically impossible when using the old-school brute force way to solve them.&lt;/p&gt;

&lt;p&gt;Even after applying some well-known techniques to shrink the problem space, the computation still takes a long time (too long for me to stick around and wait it to finish). I tried using the &lt;a href=&#34;http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes&#34;&gt;sieve of Eratosthenes&lt;/a&gt;, and the performance isn’t satisfying to say the least.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-erlang&#34;&gt;solve() -&amp;gt;
    sum_all_primes_under(2000000).

sum_all_primes_under(To) -&amp;gt;
    sieve(lists:seq(2, To), To).

sieve([], _) -&amp;gt; 0;
sieve(List, Range) -&amp;gt;
    [Max|_] = lists:reverse(List),
    case Max  List;
        false -&amp;gt;
            [H|Rest] = List,
            io:format(&amp;quot;~p ~n&amp;quot;, [H]),
            H + sieve(lists:filter(fun(X)-&amp;gt;X rem H =/= 0 end, Rest), Range)
    end.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One thing to take note is that when executing the above program, only one core of the CPU is effectively utilized.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://blog.idempotent.ca/images/cpu1.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;CPU chart before&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;h2 id=&#34;concurrency-oriented-programming&#34;&gt;Concurrency Oriented Programming&lt;/h2&gt;

&lt;p&gt;In the concurrency oriented programming (COP, a term coined by Erlang’s inventor Joe Armstrong) model, you’re thinking in terms of separate entities and what responsibility is there for each of them. Think of them as individual human beings. They don’t have shared brains; one cannot influence the other by simply modify his or her brain tissues (well, under normal circumstance anyway). As human beings, our brains are disconnected from each other, &lt;strong&gt;but&lt;/strong&gt; we share our knowledge through communication. We can modify other people’s memories by sending messages and vice versa. This is essentially what COP is – the entities are processes, and each has individual memories that’s not modifiable by other processes. Processes communicate with each other through message sending/receiving. This model is particularly good for scaling. Because the entities share nothing, it’s easy to get more entities without worrying that they don’t get along.&lt;/p&gt;

&lt;p&gt;Back to our problem. With the traditional approach, one process is to iterate through the 2 million numbers, determine if it’s a prime number, and add to the tally if it is. Think of it as one man doing all the job that there is.&lt;/p&gt;

&lt;p&gt;With the new model, here are the actors in our system:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Worker&lt;/strong&gt;: A worker is responsible for gathering the prime numbers in a given range (sufficiently small), add them and report them to the tallier.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tallier&lt;/strong&gt;: A tallier waits for the results from the workers and sum the results up.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Manager (Solver)&lt;/strong&gt;: The manager is responsible for dividing up the task, hiring workers to finish the task and tell the tallier which worker has been given a task so that the tallier can wait for the specific workers to send back the results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-erlang&#34;&gt;worker() -&amp;gt;
    receive
        {TallierPid, L} -&amp;gt;
            Sum = lists:sum(lists:filter(fun(X)-&amp;gt;mylib:is_prime(X) end, L)),
            TallierPid ! {self(), Sum}
    end.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The worker code is quite simple. After it’s created (hired by the manager), it waits for the manager to send over &lt;code&gt;{TallierPid, L}&lt;/code&gt;, with &lt;code&gt;L&lt;/code&gt; being the list of integers to work with. Then it filters out the prime numbers, and call &lt;code&gt;lists:sum()&lt;/code&gt; on the result list, which gives the sum of the prime numbers in the list &lt;code&gt;L&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This doesn’t seem to look different if we were writing in the traditional way. Yes, but feeding the key difference here is that &lt;strong&gt;we’re not going to ask one worker to solve the entire problem domain&lt;/strong&gt;. Instead, the manager is going to &lt;strong&gt;divide the task into smaller tasks so that it takes almost no time for a single worker to finish his or her work&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-erlang&#34;&gt;solve() -&amp;gt;
    TallierPid = spawn(euler, tallier, [self(), [], 0, false]),
    solver(TallierPid, 2, 2000000),
    %% worker assignment has finished
    TallierPid ! { self(), true },

    receive
        {TallierPid, Result} -&amp;gt;
            io:format(&amp;quot;Final tally: ~p ~n&amp;quot;, [Result])
    end.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is only half of the manager code. First it spawns a tallier. We’ll look at the tallier code later.
&lt;code&gt;solver(TallierPid, 2, 2000000)&lt;/code&gt; is the worker assignment part, which is shown directly after this section.&lt;/p&gt;

&lt;p&gt;After the worker assignment, the manager simply informs the tallier that the worker assignment has done, and sits there, waiting for the tallier to report the final result.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-erlang&#34;&gt;solver(TallierPid, From, To) -&amp;gt;
    case (To - From) of
         SmallRange when SmallRange =&amp;lt; 1000 -&amp;gt;
            List = lists:seq(From, To),
            WorkerPid = spawn(fun worker/0),
            %% register the worker with tallier
            TallierPid ! { self(), WorkerPid },
            %% send the problem for the worker to work out
            WorkerPid ! { TallierPid, List };
        _ -&amp;gt;
            Mid = mylib:floor((To + From) div 2),
            solver(TallierPid, From, Mid),
            solver(TallierPid, Mid+1, To)
    end.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code defines the problem domain, splits the work, hire workers and assign the split work to them. It’s a classic divide-and-conquer structure, however, we have to be careful not to make the range too small, as the number of processes may (or in this case, will) exceed the default system limit. As I tested, my computer can sum prime numbers from a list of 1000 elements in a heartbeat so I just chose that number to be the upper bound of the size for each worker to work on.&lt;/p&gt;

&lt;p&gt;Finally,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-erlang&#34;&gt;tallier(SolverPid, Workers, X, AssignFinished) -&amp;gt;

    receive
        {SolverPid, true} -&amp;gt;
            %% This message is from the solver
            %% It tells the tallier that
            %% worker assignment has finished
            tallier(SolverPid, Workers, X, true);
        {SolverPid, WorkerPid} -&amp;gt;
            %% This message is from the solver
            %% Register WorkerPid
            tallier(SolverPid, [WorkerPid | Workers], X, AssignFinished);
        {WorkerPid, Sum} -&amp;gt;
            Tally = X + Sum,
            %% Remove worker from Workers list
            NewWorkerList = Workers -- [WorkerPid],
            case AssignFinished of
                true -&amp;gt;
                    case NewWorkerList of
                        [] -&amp;gt;
                            SolverPid ! { self(), Tally };
                        _ -&amp;gt; tallier(SolverPid, NewWorkerList, Tally, AssignFinished)
                    end;
                _ -&amp;gt; tallier(SolverPid, NewWorkerList, Tally, AssignFinished)
            end
    end.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The tallier keeps track of 4 “variables”:
1. The Solver’s ID – to recognize messages from the solver and to report final tally to the solver.
2. A list of &lt;code&gt;Worker&lt;/code&gt;s’ IDs whose results have been reported yet.
3. &lt;code&gt;X&lt;/code&gt; – The previous tally.
4. &lt;code&gt;AssignFinished&lt;/code&gt; – a flag indicating that all workers have been assigned.&lt;/p&gt;

&lt;p&gt;The tallier has to look after three types of messages:
1. Message from the manager that the job has been divided up and the workers have been dispatched. In this case, the tallier knows that when the &lt;code&gt;Worker&lt;/code&gt;s list gets down to empty, the computation is finished.
2. Message from the manager that registers a worker with the tallier. The tallier adds this worker’s ID to the &lt;code&gt;Worker&lt;/code&gt;s list and waits for that worker to return his or her result.
3. Message from the worker indicating the worker’s ID and the result. The tallier removes the worker’s ID from the waiting pool and adds the result to the tally. When the waiting pool is empty and &lt;code&gt;AssignFinished&lt;/code&gt; is true, it signals the end of the job and thus the tallier reports the final tally to the manager (solver).&lt;/p&gt;

&lt;p&gt;There we go!  Running the program on my work computer (QuadCore) gives me the correct result in less than 5 seconds. About 8 seconds for my laptop (which is a Core 2 Duo). Non of this is even remotely imaginable with the traditional approach.&lt;/p&gt;

&lt;p&gt;Also, as you may have guessed, all cores on the CPU are fully utilized.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://blog.idempotent.ca/images/cpu2.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;CPU chart after&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Programming in Erlang is fun!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>